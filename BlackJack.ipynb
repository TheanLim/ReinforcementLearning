{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlackJack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K2lLlK_O4ETp",
        "JdxPUFNB1WqQ"
      ],
      "authorship_tag": "ABX9TyPOC03oJlRhXelN/kL0zELI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheanLim/ReinforcementLearning/blob/master/BlackJack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASMv1byExcb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2lLlK_O4ETp",
        "colab_type": "text"
      },
      "source": [
        "# Brief Intro on BlackJack in OpenGym AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADmeUdq116k",
        "colab_type": "text"
      },
      "source": [
        "The definition of Blackjack-v0 is found in here https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\n",
        "\n",
        "> Blackjack is a card game where the goal is to obtain cards that sum to as\n",
        "    near as possible to 21 without going over.  They're playing against a fixed\n",
        "    dealer.\n",
        "    Face cards (Jack, Queen, King) have point value 10.\n",
        "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
        "    This game is placed with an infinite deck (or with replacement).\n",
        "    The game starts with *dealer having one face up and one face down card*, while\n",
        "    player having two face up cards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqONpqw1gd9",
        "colab_type": "code",
        "outputId": "f1eadad3-1356-4d36-9354-924f733edd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import gym\n",
        "env = gym.make('Blackjack-v0') # Create the Blackjack Envirionment\n",
        "obs = env.reset()  # Use.reset() to initialize and get the first observation\n",
        "print(obs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 5, False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAoR1R1_3kpW",
        "colab_type": "text"
      },
      "source": [
        "The observation is a 3-tuple of: \n",
        "* the players current sum,\n",
        "* the dealer's one showing card (1-10 where 1 is ace),\n",
        "* and whether or not the player holds a usable ace (0 or 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lStu1Wps2-Xa",
        "colab_type": "code",
        "outputId": "433b1e1f-06eb-4269-c65d-14643df22b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UC54zBz3BVC",
        "colab_type": "text"
      },
      "source": [
        "`Discrete(2)` means that the possible actions are integers 0 and 1.\n",
        "  * The player can request additional cards (hit=1) until they decide to stop\n",
        "    (stick=0) or exceed 21 (bust)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sB1MUg44H3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assuming we choose to hit and we take a step\n",
        "action = 1\n",
        "obs, rewards, done, info = env.step(action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhkmYxNm4V9K",
        "colab_type": "code",
        "outputId": "4b95a2a8-f338-46fa-a2ad-10d000d80193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(obs)\n",
        "print(rewards)\n",
        "print(done)\n",
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 5, True)\n",
            "0.0\n",
            "False\n",
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grp0k3vi58c-",
        "colab_type": "text"
      },
      "source": [
        "* `obs` -- \n",
        "The original observation was `(9, 5, False)` and it changed into `(20, 5, True)` after we decided to hit. \n",
        "  * We got an Ace (reusable) because we are seeing `True`\n",
        "  * Dealer's sum remains unchanged. This makes sense because our decision doesn't depend on Dealer's facedown card.\n",
        "*`rewards` --\n",
        "The reward for winning is +1, drawing is 0, and losing is -1.\n",
        "*`done` -- The game is not done yet because (1) we are not busted, (2) we didn't stick\n",
        "*`info` -- Not relevant in this environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-AZoTpnSC7Y",
        "colab_type": "text"
      },
      "source": [
        "# Intro to Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa9fFTLYSHYN",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/agents/tutorials/0_intro_rl "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIk51OaqELg3",
        "colab_type": "text"
      },
      "source": [
        "# tf Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFn5ZZj6TdVw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ7SGZKvQXyY",
        "colab_type": "text"
      },
      "source": [
        "## System Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YmlHu0vEOkc",
        "colab_type": "text"
      },
      "source": [
        "![System Overview](https://drive.google.com/uc?id=1bOWE4DAiAcJDZ19NusM3juLyVUrmQT3H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oll6jLv1NOfl",
        "colab_type": "text"
      },
      "source": [
        "A TF-Agents training program is usually split into two parts that run in parallel:\n",
        "\n",
        "\n",
        "> On the left, a `driver` explores the `environment`(task) using a `collect policy` to choose actions, and it collects `trajectories` (i.e., experiences), sending them to an observer, which saves them to a `replay buffer`; \n",
        "\n",
        ">On the right, an `agent` pulls batches of `trajectories` from the `replay buffer` and trains some `networks`, which the `collect policy` uses. \n",
        "\n",
        "In short, the left part explores the `environment` and collects `trajectories`, while the right part learns and updates the `collect policy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eegVtJOCMf",
        "colab_type": "text"
      },
      "source": [
        "1.   **Multiple Environments** - You'd want to explore multiple copies of environments in parallel to (1) use all of the resources (CPU and GPU) available (2) create trajectories (experiences) that are less correlated during training.\n",
        "2.   **Trajectories** are a sequence of consecutive transitions from time step *n* to time step *n + t*.\n",
        "3.   An **observer** is just any function that takes a trajectory as an argument. It may seem redundant but it allows flexibility. You can:\n",
        "  1. Use an observer to save trajectories into replay buffer or to a file\n",
        "  2. Compute metrics using trajectories\n",
        "  3. Pass multiple observers to the driver and broadcast trajectories to all of them\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX5bOtL6QkoI",
        "colab_type": "text"
      },
      "source": [
        "## Basic Components\n",
        "\n",
        "Let's look at a DQN Network first.\n",
        "\n",
        "The components are (in sequence): \n",
        "1. `Deep Q-Network`\n",
        "2. `DQN agent` (which will take care of creating the `collect policy`)\n",
        "3. `Replay Buffer` and the `observer`\n",
        "4. `Training Metrics`\n",
        "5. `Driver`\n",
        "6. `Dataset` - `tf.data.Dataset`\n",
        "7. Populate the `replay buffer`\n",
        "8. `train`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WabIiDDDMSXx",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "import tensorflow as tf\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "  train_env.observation_spec(),\n",
        "  train_env.action_spec(),\n",
        "  fc_layer_params=(100,))\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "  train_env.time_step_spec(),\n",
        "  train_env.action_spec(),\n",
        "  q_network=q_net,\n",
        "  optimizer=optimizer,\n",
        "  td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "  train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent.initialize()\n",
        "```\n",
        "An example of an optimizer is the AdamOptimizer:\n",
        "\n",
        "`optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxPUFNB1WqQ",
        "colab_type": "text"
      },
      "source": [
        "# Ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETjq1YJ-zI5u",
        "colab_type": "text"
      },
      "source": [
        "To activate virtual display we need to run a script once for training an agent, as follows:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGxKt5mAzJGT",
        "colab_type": "code",
        "outputId": "c4745093-a3df-4884-b531-47328c73aebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# For rendering Environment\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pigletfrom pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gS42JAzFMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UPYUP4JzOlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7By1ebQAzRqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NYaqOV0HuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}